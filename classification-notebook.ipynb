{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEADLINER: Real or Fake News?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real1 = pd.read_csv('./real-news/realnews4.csv')\n",
    "real2 = pd.read_csv('./real-news/realnews5.csv')\n",
    "real = pd.concat([real1, real2])\n",
    "real['class'] = 'real'\n",
    "print('The shape of the concatenated real file is', real.shape)\n",
    "real = real.drop_duplicates(subset=['text'], keep='first')\n",
    "real = real.dropna(subset=['text'])\n",
    "real = real.reset_index(drop=True)\n",
    "print('After dropping null text and duplicate text, the shape of the concatenated real file is', real.shape)\n",
    "\n",
    "sites = list(real['site'])\n",
    "real_sites = ['cnn', 'politico', 'abcnews.go', 'google', 'bbc.com', 'economist', 'nytimes', 'pbs', 'cbs', 'nbcnews', 'bloomberg', 'npr', 'c-span', 'independent', 'apnews', 'thehill', 'fivethirtyeight', 'forbes', 'money.cnn']\n",
    "ind = []\n",
    "for j in range(len(sites)):\n",
    "    if any(substring in sites[j] for substring in real_sites) == True:\n",
    "        ind.append(j)\n",
    "    else:\n",
    "        pass\n",
    "real = real[real.index.isin(ind)]\n",
    "real = real.reset_index(drop=True)\n",
    "print('After keeping rows with credible site urls, the shape of the real file is', real.shape )\n",
    "real.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake = pd.read_csv('../Data_sets/fake.csv')\n",
    "print('The shape of the data is', fake.shape)\n",
    "fake['class'] = 'fake'\n",
    "fake = fake.drop_duplicates(subset=['text'], keep='first')\n",
    "fake = fake.dropna(subset=['text'])\n",
    "fake = fake.reset_index(drop=True)\n",
    "print('After dropping null text and duplicate text, the shape of the fake file is', fake.shape)\n",
    "fake.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_subset = real.loc[:, [\"author\", \"title\", \"published\", \"site\", \"text\", \"main_image\", \"shares\", \"ord_in_thread\", \"spam_score\", \"participants_count\", \"replies_count\", \"likes\", \"class\"]]\n",
    "fake_subset = fake.loc[:, [\"author\", \"title\", \"published\", \"site_url\", \"text\", \"main_img_url\", \"shares\", \"ord_in_thread\", \"spam_score\", \"participants_count\", \"replies_count\", \"likes\", \"class\"]]\n",
    "fake_subset = fake_subset.rename(index=str, columns={\"site_url\": \"site\", \"main_img_url\": \"main_image\"})\n",
    "data = pd.concat([real_subset, fake_subset])\n",
    "print('The shape of the concatenated dataset is', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "words = list(chain(data['text']))\n",
    "text = []\n",
    "for i in range(len(words)):\n",
    "    w = words[i].decode('utf-8')\n",
    "    w = w.encode('ascii', 'ignore')\n",
    "    text.append(w)\n",
    "data['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.to_csv('./real_fake_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary lists \n",
    "Created a list of top 1000 and top 000 frequency occuring words in fake and real news feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./real_fake_data.csv')\n",
    "data = data.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_data = list(data['text'][data['class'] == 'real'])\n",
    "fake_data = list(data['text'][data['class'] == 'fake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def parse_top_vocab(data):\n",
    "    count_vectorizer = CountVectorizer(stop_words='english', strip_accents='unicode', decode_error = 'ignore')\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    \n",
    "    term_counts = count_vectorizer.fit_transform(data)\n",
    "    term_names = count_vectorizer.get_feature_names()\n",
    "    \n",
    "    term_tfidf = tfidf_transformer.fit_transform(term_counts)\n",
    "    \n",
    "    pairs = dict(zip(term_names, np.asarray(term_tfidf.mean(axis=0)).ravel()))\n",
    "    sorted_all = sorted(pairs.items(), key=lambda x: -x[1])\n",
    "    sorted_top_1000 = sorted_all[:1000]\n",
    "    sorted_top_7000 = sorted_all[:7000]\n",
    "    \n",
    "    top_1000 = [(i[0]).encode('ascii', 'ignore') for i in sorted_top_1000]\n",
    "    top_7000 = [(i[0]).encode('ascii', 'ignore') for i in sorted_top_7000]\n",
    "    return(top_1000, top_7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_vocab_1000, real_vocab_7000 = parse_top_vocab(real_data)\n",
    "fake_vocab_1000, fake_vocab_7000 = parse_top_vocab(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Naive Bayes Classifier using fake_vocab_7000 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5768, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data = data[data['class'] == 'real']\n",
    "fake_data = data[data['class'] == 'fake'].sample(n=3000, random_state=0)\n",
    "data = pd.concat([real_data, fake_data])\n",
    "data = data.reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['class'], test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4326, 7000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "train_count_vectorizer = CountVectorizer(strip_accents='unicode', decode_error = 'ignore', stop_words='english', vocabulary=fake_vocab_7000)\n",
    "train_term_counts = train_count_vectorizer.fit_transform(X_train.values)\n",
    "train_term_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer_final.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(train_count_vectorizer, 'vectorizer_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4326, 7000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "train_tfidf = TfidfTransformer()\n",
    "train_tfidf_output = train_tfidf.fit_transform(train_term_counts)\n",
    "train_tfidf_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1442, 7000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_count_vectorizer = CountVectorizer(strip_accents='unicode', decode_error = 'ignore', stop_words='english', vocabulary=fake_vocab_7000)\n",
    "test_term_counts = test_count_vectorizer.fit_transform(X_test.values)\n",
    "test_term_counts.shape\n",
    "\n",
    "test_tfidf = TfidfTransformer()\n",
    "test_tfidf_output = test_tfidf.fit_transform(test_term_counts)\n",
    "test_tfidf_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0).fit(train_tfidf_output, y_train.values)\n",
    "predicted = clf.predict(test_tfidf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85159500693481271"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test.values, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_fake</th>\n",
       "      <th>prob_real</th>\n",
       "      <th>predicted</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>0.490489</td>\n",
       "      <td>0.509511</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0.452491</td>\n",
       "      <td>0.547509</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>0.984394</td>\n",
       "      <td>0.015606</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>0.910570</td>\n",
       "      <td>0.089430</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.434261</td>\n",
       "      <td>0.565739</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prob_fake  prob_real predicted  real\n",
       "776    0.490489   0.509511      real  real\n",
       "825    0.452491   0.547509      real  real\n",
       "4979   0.984394   0.015606      fake  fake\n",
       "3583   0.910570   0.089430      fake  fake\n",
       "1095   0.434261   0.565739      real  real"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = pd.DataFrame(clf.predict_proba(test_tfidf_output), columns=['prob_fake', 'prob_real'], index=X_test.index)\n",
    "probs['predicted'] = predicted\n",
    "probs['real'] = y_test.values\n",
    "probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mnnb_model_final.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'mnnb_model_final.pkl')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}